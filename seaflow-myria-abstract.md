The analysis of large datasets is a relatively new problem for biological oceanographers. 
Instruments which collect data continuously while research vessels are underway produce large amounts of data.
Datasets made up of multiple cruises' worth of underway data quickly become an untractable data analysis problem, if the data is not stored in an optimized way.
Here we present a case study where we have applied cutting edge data science methodologies to organising and analysing oceanographic "big data".
The data set is made up of flow cytometry data from SeaFlow, an instrument which analyses seawater collected continuously from a research vessel's flow through system.
We outline an approach for storing the data in a large database system, and integrating and joining data from different underway instruments.
Next, we describe some of the queries used to filter and normalise the data, and how we used IPython notebooks for further data analysis and visualization.
Finally we discuss how this approach can be generalised to a conceptual pipeline for working more effectively with large datasets in the biological and earth sciences.
